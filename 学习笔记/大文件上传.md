## 大文件上传

前端大文件上传核心是利用 Blob.prototype.slice 方法，和数组的 slice 方法相似，文件的 slice 方法可以返回原文件的某个切片

预先定义好单个切片大小，将文件切分为一个个切片，然后借助 http 的可并发性，同时上传多个切片。

这样从原本传一个大文件，变成了并发传多个小的文件切片，可以大大减少上传时间。

```sh
yarn create vite big-upload --template vue
cd big-upload
// 安装需要的库
yarn add element-plus
```
src\main.js

```js
import { createApp } from 'vue';
import App from './App.vue';

// element-ui
import ElementPlus, { ElMessage } from 'element-plus';
import 'element-plus/lib/theme-chalk/index.css';
import locale from 'element-plus/lib/locale/lang/zh-cn'; //中文

const app = createApp(App);
// 全局挂载
app.config.globalProperties.$message = ElMessage;

app.use(ElementPlus, { size: 'small', locale }).mount('#app');
```
src\App.vue

```vue
<template>
  <h1>大文件上传</h1>
  <input type="file" @change="handleFileChange" />
  <el-button @click="handleUpload">上传</el-button>
  <el-button @click="handlePause" v-if="isPaused">暂停</el-button>
  <el-button @click="handleResume" v-else>恢复</el-button>
  <div v-show="hashPercentage > 0">
    <h3>计算文件的hash</h3>
    <el-progress :percentage="hashPercentage"></el-progress>
    {{ '计算完成，文件hash为：' + hash }}
  </div>
  <div v-show="uploadPercentage > 0">
    <h3>大文件上传总进度</h3>
    <el-progress :percentage="fakeUploadPercentage"></el-progress>
  </div>
  <div v-show="chunkList.length > 0">
    <h3>分片上传进度</h3>
    <el-table :data="chunkList" style="width: 100%">
      <el-table-column prop="chunkHash" label="分块" width="270"> </el-table-column>
      <el-table-column prop="size" label="size(Kb)" width="90" :formatter="(row, column, value) => Math.floor(value / 1024)"> </el-table-column>
      <el-table-column prop="percentage" label="上传进度">
        <template #default="scope">
          <el-progress :percentage="scope.row.percentage"></el-progress>
        </template>
      </el-table-column>
    </el-table>
  </div>
</template>

<script>
import SparkMD5 from 'spark-md5';
// 点击上传按钮时，调用 createFileChunk 将文件切片，切片数量通过文件大小控制，这里设置 3MB，也就是说一个 30 MB 的文件会被分成 10 个 3MB 的切片
const SIZE = 3 * 1024 * 1024; // 定义切片的大小
const Status = { wait: 1, error: 2, done: 3, fail: 4 }; // 定义文件切片状态

export default {
  data() {
    return {
      // 上传的文件
      file: null, // 初始值需要是 null  空对象 if判断的时候是true
      // 文件分片之后  每一个片
      chunkList: [],
      worker: null,
      hash: '',
      hashPercentage: 0,
      // 创建一个“假”的进度条，这个假进度条基于文件进度条，但只会停止和增加
      fakeUploadPercentage: 0,
      requestList: [],
      isPaused: true,
    };
  },
  computed: {
    // 针对每一个 chunk的进度 计算出总的上传进度
    // 将每个切片已上传的部分累加，除以整个文件的大小，就能得出当前文件总的上传进度
    uploadPercentage() {
      if (!this.file || !this.chunkList.length) return 0;
      const loaded = this.chunkList.map((item) => item.size * item.percentage).reduce((acc, cur) => acc + cur);
      return parseInt((loaded / this.file.size).toFixed(2));
    },
  },
  watch: {
    uploadPercentage(now) {
      if (now > this.fakeUploadPercentage) {
        // 文件进度是这个，暂停会取消并清空切片的 xhr 请求，此时如果已经上传了一部分，就会发现文件进度条有倒退的现象
        // 点击恢复时，由于重新创建了 xhr 导致切片进度清零，总进度条会倒退
        // 在这里保证进度条不倒退，给用户更好的体验
        this.fakeUploadPercentage = now;
      }
    },
  },
  methods: {
    // 选择文件的时候会触发这个事件， e.target.files 表示选择的文件列表
    handleFileChange(e) {
      const [file] = e.target.files;
      if (!file) {
        this.file = null;
        return;
      }
      this.file = file;
    },
    //  上传按钮点击事件
    async handleUpload() {
      if (!this.file) {
        this.$message.info('请选择一个文件吧');
        return;
      }
      this.resetData();
      // 获取文件分片数组
      const fileChunkList = this.createFileChunk(this.file);
      // 断点续传的原理在于前端/服务端需要记住已上传的切片，这样下次上传就可以跳过之前已上传的部分，
      // 文件名一旦修改就失去了效果，而事实上只要文件内容不变，hash 就不应该变化，所以正确的做法是根据文件内容生成 hash
      this.hash = await this.calculateHash(fileChunkList);
      // 文件秒传，即在服务端已经存在了上传的资源，所以当用户再次上传时会直接提示上传成功 
      // 在上传前，先计算出文件 hash，并把 hash 发送给服务端进行验证，验证文件是否已上传过，根据 hash 判断
      const { shouldUpload, uploadedList } = await this.verifyUpload(this.file.name, this.hash);
      if (!shouldUpload) {
        this.$message.success('秒传：上传成功');
        return;
      }
      // 构建 chunkList，为每一个切片添加标识以及上传进度(是每一个chunk的上传进度)
      this.chunkList = fileChunkList.map(({ file }, index) => ({
        // 对应 file.slice 返回的切片
        chunk: file,
        size: file.size,
        // 给每个切片一个标识作为 hash。这样后端可以知道当前切片是第几个切片，用于之后的合并切片。判断哪些切片未上传成功，确保重传有效。
        chunkHash: `${this.hash}-${index}`,
        // 文件的 hash 值
        fileHash: this.hash,
        index,
        // 当前块上传的进度，在点击上传/恢复上传时，会调用验证接口返回已上传的切片，所以需要将已上传切片的进度变成 100%
        // 遍历所有切片时判断当前切片是否在已上传列表里即可
        percentage: uploadedList.includes(`${this.hash}-${index}`) ? 100 : 0, 
      }));
      // 上传 chunk
      await this.uploadChunks(uploadedList);
      this.$message.success('上传成功');
    },
    resetData() {
      this.chunkList = [];
      this.worker = null;
      this.hash = '';
      this.hashPercentage = 0;
      this.requestList = [];
      this.isPaused = true;
    },
    // 生成文件切片数组，createFileChunk 内使用 while 循环和 slice 方法将切片放入 fileChunkList 数组中
    createFileChunk(file, size = SIZE) {
      const fileChunkList = [];
      let cur = 0;
      while (cur < file.size) {
        // file.slice 返回一个 blob对象
        fileChunkList.push({ file: file.slice(cur, cur + size) });
        cur += size;
      }
      return fileChunkList;
    },
    // 抽样hash
    // 文件切成 2 M的切片，抽取文件头尾2mb, 中间的部分取首中尾三个地方各2个字节，将这些片段组合成新的 buffer,进行 md5 计算
    calculateHash() {
      return new Promise((resolve) => {
        const spark = new SparkMD5.ArrayBuffer();
        const reader = new FileReader();
        const file = this.file;
        // 文件大小
        const size = this.file.size;
        // 取样范围 2MB
        let offset = 2 * 1024 * 1024;
        // 前面 2MB
        let chunks = [file.slice(0, offset)];
        let cur = offset;
        while (cur < size) {
          // 最后一块全部加进来
          if (cur + offset >= size) {
            // 后面 2MB
            chunks.push(file.slice(cur, cur + offset));
          } else {
            // 中间的前中后各取2个字节
            const mid = cur + offset / 2;
            const end = cur + offset;
            chunks.push(file.slice(cur, cur + 2));
            chunks.push(file.slice(mid, mid + 2));
            chunks.push(file.slice(end - 2, end));
          }
          cur += offset;
        }
        // 拼接
        reader.readAsArrayBuffer(new Blob(chunks));
        // 读取文件切片
        reader.onload = (e) => {
          spark.append(e.target.result);
          this.hashPercentage = 100;
          // 返回最终文件的 hash 值
          resolve(spark.end());
        };
      });
    },
    // 判断文件是否可以秒传
    // 前端每次上传前发送一个验证的请求，返回两种结果
    // 1、服务端已存在该文件，不需要再次上传
    // 2、服务端不存在该文件或者已上传部分文件切片，通知前端进行上传，并把已上传的文件切片返回给前端
    async verifyUpload(filename, fileHash) {
      const { data } = await this.request({
        url: 'http://localhost:8080/verify',
        method: 'post',
        headers: { 'content-type': 'application/json' },
        data: JSON.stringify({ filename, fileHash }),
      });
      return JSON.parse(data).data;
    },
    // 对原生的 XMLHttpRequest 进行了一层封装
    request({ url, method = 'post', data, headers = {}, onProgress = (e) => e, requestList = [] }) {
      return new Promise((resolve, reject) => {
        const xhr = new XMLHttpRequest();
        // 监听上传进度
        xhr.upload.onprogress = onProgress;
        xhr.open(method, url);
        Object.keys(headers).forEach((key) => xhr.setRequestHeader(key, headers[key]));
        xhr.send(data);
        xhr.onload = (e) => {
          if (requestList) {
            // 将请求成功的 xhr 从列表中删除
            const xhrIndex = requestList.findIndex((item) => item === xhr);
            requestList.splice(xhrIndex, 1);
          }
          resolve({ data: e.target.response });
        };
        xhr.onreadystatechange = function () {
          if (xhr.readyState === 4) {
            if (xhr.status === 200) {
              // 响应成功
            } else {
              // 控制进度
              onProgress({ loaded: 0, total: 100 });
              // 错误处理
              reject(xhr.statusText);
            }
          }
        };
        //  保存所有的 xhr，将上传每个切片的 xhr 对象保存起来，用于实现“断点”，也就是暂停上传
        requestList?.push(xhr);
      });
    },
    // 上传文件切片
    async uploadChunks(uploadedList = []) {
      // 构造请求列表
      const requestList = this.chunkList
         // 过滤已上传的切片，用于恢复上传
        .filter((chunk) => !uploadedList.includes(chunk.chunkHash))
        .map(({ chunk, chunkHash, index, fileHash }) => {
          const formData = new FormData();
          // 将文件切片，切片 hash，以及文件 hash 放入 formData 中，
          formData.append('chunk', chunk);
          formData.append('chunkHash', chunkHash);
          formData.append('fileHash', fileHash);
          return { formData, index };
        });
      // 控制并发
      await this.sendRequest(requestList, 4);
      // chunk 全部发送完成了需要通知后端合并切片
      // 之前上传的切片数量 + 本次上传的切片数量 = 所有切片数量时合并切片
      if (uploadedList.length + requestList.length === this.chunkList.length) {
        await this.mergeRequest();
      }
    },
    // 控制并发数量，解决文件切片过多导致并发 http 请求过多问题  max表示最大的并发数
    async sendRequest(forms, max = 4) {
      return new Promise((resolve, reject) => {
        const len = forms.length;
        let counter = 0; // 已经发送成功的请求
        const retryArr = []; // 记录错误的次数
        // 一开始将所有的文件切片状态置为等待
        forms.forEach((item) => (item.status = Status.wait));
        const start = async () => {
          // 有请求，有通道
          while (counter < len && max > 0) {
            max--; // 占用通道
            // 只要是没有完成的我们就重发
            let idx = forms.findIndex((v) => v.status == Status.wait || v.status == Status.error);
            if (idx === -1) {
              // 连失败状态和等待状态都没有，找不到失败状态和等待状态
              return reject();
            }
            let { formData, index } = forms[idx];
            await this.request({
              url: 'http://localhost:8080/upload-chunk',
              method: 'post',
              data: formData,
              // 监听每个切片的上传进度
              onProgress: this.createProgressHandler(this.chunkList[index]),
              requestList: this.requestList,
            })
              .then(() => {
                forms[idx].status = Status.done;
                max++; // 释放通道
                counter++;
                if (counter === len) {
                  resolve();
                }
              })
              .catch(() => {
                forms[idx].status = Status.error;
                if (typeof retryArr[index] !== 'number') {
                  this.$message.info(`第 ${index} 个块上传失败，系统准备重试`);
                  retryArr[index] = 0;
                }
                // 次数累加
                retryArr[index]++;
                // 一个请求报错3次的
                if (retryArr[index] > 3) {
                  this.$message.error(`第 ${index} 个块重试多次无效，放弃上传`);
                  forms[idx].status = Status.fail;
                }
                max++; // 释放通道
              });
          }
        };
        start();
      });
    },
    //  chunkList 每一项设置百分比
    createProgressHandler(item) {
      return (e) => {
        item.percentage = parseInt(String((e.loaded / e.total) * 100));
      };
    },
    // 通知后端合并切片 前端在发送合并请求时会携带文件名，服务端根据文件名可以找到上一步创建的切片文件夹
    async mergeRequest() {
      await this.request({
        url: 'http://localhost:8080/merge',
        method: 'post',
        headers: { 'content-type': 'application/json' },
        data: JSON.stringify({ filename: this.file.name, fileSize: this.file.size, size: SIZE, hash: this.hash }),
      });
    },
    // 实现暂停上传
    // 原理是使用 XMLHttpRequest 的 abort 方法，可以取消一个 xhr 请求的发送
    handlePause() {
      this.requestList.forEach((xhr) => xhr?.abort());
      this.requestList = [];
      this.isPaused = false;
    },
    // 恢复
    // 文件切片上传后，服务端会建立一个文件夹存储所有上传的切片，
    // 所以每次前端上传前可以调用一个接口，服务端将已上传的切片的切片名返回，前端再跳过这些已经上传切片，这样就实现了“续传”的效果
    async handleResume() {
      this.isPaused = true;
      const { uploadedList } = await this.verifyUpload(this.file.name, this.hash);
      await this.uploadChunks(uploadedList);
    },
  },
};
</script>

<style></style>
```

服务端负责接受前端传输的切片，并在接收到所有切片后合并所有切片。

这里引伸出两个问题

1. 何时合并切片，即切片什么时候传输完成
2. 如何合并切片

第一个问题，何时合并切片?

需要前端在每个切片中都携带切片最大数量的信息，当服务端接受到这个数量的切片时自动合并。

或者也可以额外发一个请求，主动通知服务端进行切片的合并

第二个问题，如何合并切片呢？

可以使用 Nodejs 的 读写流（readStream/writeStream），将所有切片的流传输到最终文件的流里

```sh
// koa2脚手架
npm install koa-generator -g
// 脚手架创建项目
koa2 server
cd server
yarn
// 安装对应的库
yarn add koa-body fs-extra
// 删去一些不需要使用的文件  全局引入koa-body 并且配置  创建upload路由
```

server\app.js

```js
const Koa = require('koa');
const app = new Koa();
const json = require('koa-json');
const onerror = require('koa-onerror');
const logger = require('koa-logger');
const koaBody = require('koa-body');

const upload = require('./routes/upload');
require('./schedule/chunkClearShedule');

// error handler
onerror(app);

// middlewares
app.use(json());
app.use(logger());
app.use(require('koa-static')(__dirname + '/public'));

// koa-body
app.use(koaBody({ multipart: true, formidable: { maxFileSize: 200 * 1024 * 1024 } }));

// logger
app.use(async (ctx, next) => {
  const start = new Date();
  await next();
  const ms = new Date() - start;
  console.log(`${ctx.method} ${ctx.url} - ${ms}ms`);
});

// 跨域
app.use(async (ctx, next) => {
  ctx.set('Access-Control-Allow-Origin', '*');
  ctx.set('Access-Control-Allow-Headers', 'Content-Type, Content-Length, Authorization, Accept, X-Requested-With , yourHeaderFeild');
  if (ctx.method == 'OPTIONS') {
    ctx.status = 200;
  } else {
    await next();
  }
});

// routes
app.use(upload.routes(), upload.allowedMethods());

// error-handling
app.on('error', (err, ctx) => {
  console.error('server error', err, ctx);
});

module.exports = app;
```

server\routes\upload.js

```js
const router = require('koa-router')();
const path = require('path');
const fse = require('fs-extra');

// 提取文件后缀名
const extractExt = (filename) => filename.slice(filename.lastIndexOf('.'), filename.length);
// 大文件存储目录
const UPLOAD_DIR = path.resolve(__dirname, '..', 'target');

// 返回已经上传切片名列表
const createUploadedList = async (fileHash) =>
  fse.existsSync(path.resolve(UPLOAD_DIR, `${fileHash}-chunks`)) ? await fse.readdir(path.resolve(UPLOAD_DIR, `${fileHash}-chunks`)) : [];

/**
 * 针对 path 创建 readStream 并写入 writeStream,写入完成之后删除文件
 * @param {String} path
 * @param {String} writeStream
 */
const pipeStream = (path, writeStream) =>
  new Promise((resolve) => {
    const readStream = fse.createReadStream(path);
    readStream.on('end', () => {
      // fse.unlinkSync(path);
      resolve();
    });
    readStream.pipe(writeStream);
  });

/**
 * 读取所有的 chunk 合并到 filePath 中
 * @param {String} filePath 文件存储路径
 * @param {String} chunkDir chunk存储文件夹名称
 * @param {String} size 每一个chunk的大小
 */
async function mergeFileChunk(filePath, chunkDir, size) {
  // 获取chunk列表
  const chunkPaths = await fse.readdir(chunkDir);
  // 根据切片下标进行排序  否则直接读取目录的获得的顺序可能会错乱
  chunkPaths.sort((a, b) => a.split('-')[1] - b.split('-')[1]);
  await Promise.all(
    chunkPaths.map((chunkPath, index) =>
      pipeStream(
        path.resolve(chunkDir, chunkPath),
        // 指定位置创建可写流
        fse.createWriteStream(filePath, {
          start: index * size,
          end: (index + 1) * size,
        })
      )
    )
  );
  // fse.rmdirSync(chunkDir); // 合并后删除保存切片的目录
}

// 上传 chunk
router.post('/upload-chunk', async (ctx, next) => {
  // 模拟请求失败
  // if (Math.random() > 0.5) {
  // ctx.status = 500;
  // return;
  // }
  // 请求参数在这
  // hash是当前chunk的唯一值   fileHash是文件的hash
  const { chunkHash, fileHash } = ctx.request.body;
  // 文件在这
  const { chunk } = ctx.request.files;
  // chunks保存路径是 filename-chunks (使用hash可以废弃了)
  // const chunkDir = path.resolve(UPLOAD_DIR, `${filename}-chunks`);
  const chunkDir = path.resolve(UPLOAD_DIR, `${fileHash}-chunks`);
  // 切片目录不存在，创建切片目录
  if (!fse.existsSync(chunkDir)) {
    await fse.mkdirs(chunkDir);
  }

  // fs-extra 专用方法，类似 fs.rename 并且跨平台
  // fs-extra 的 rename 方法 windows 平台会有权限问题
  // https://github.com/meteor/meteor/issues/7852#issuecomment-255767835
  await fse.move(chunk.path, `${chunkDir}/${chunkHash}`);
  ctx.body = { code: 0, data: '', msg: '上传成功' };
});

// 合并
router.post('/merge', async (ctx, next) => {
  const { filename, fileSize, size, hash } = ctx.request.body;
  // filePath 是文件存储路径
  const ext = extractExt(filename);
  const filePath = path.resolve(UPLOAD_DIR, `${hash}${ext}`);
  const chunkDir = path.resolve(UPLOAD_DIR, `${hash}-chunks`);
  await mergeFileChunk(filePath, chunkDir, size);
  ctx.body = { code: 0, data: '', msg: '合并成功' };
});

// 验证是否存在
router.post('/verify', async (ctx, next) => {
  const { filename, fileHash } = ctx.request.body;
  console.log('==>', filename);
  console.log('==>', fileHash);
  let shouldUpload = true;
  let msg = '文件不存在，需要上传';
  const ext = extractExt(filename);
  const filePath = path.resolve(UPLOAD_DIR, `${fileHash}${ext}`);
  if (fse.existsSync(filePath)) {
    shouldUpload = false;
    msg = '文件存在，不需要上传';
  }
  ctx.body = { code: 0, data: { shouldUpload, uploadedList: await createUploadedList(fileHash) }, msg };
});

module.exports = router;
/**
 * 有一个坑  就是存文件切片的文件夹名称和最后保存文件的文件名称是一致的
 * 系统是不允许有同名的文件和文件夹，就导致合并chunk的时候会卡住 也不报错
 *
 * 特此规定，保存chunk的文件就名称后面需要添加 -chunk后缀以和文件名区分
 */
```

server\schedule\chunkClearShedule.js

```js
const fse = require('fs-extra');
const path = require('path');
const schedule = require('node-schedule');
// 大文件存储目录
const UPLOAD_DIR = path.resolve(__dirname, '..', 'target');

// 空目录删除
function remove(file, stats) {
  const now = new Date().getTime();
  const offset = now - stats.ctimeMs;
  if (offset > 1000 * 60) {
    // 大于60秒的碎片
    fse.unlinkSync(file);
    console.log(file, '文件已过期，删除完毕');
  }
}
// 有时上传大文件失败了，导致这个大文件因为没有完全上传成功就没有进行合并和 chunk 清理
// 检测文件的改动时间，一段时间没有改动就认为这个chunk是用户不需要的，主动清理它
async function scan(dir, callback) {
  const files = fse.readdirSync(dir);
  files.forEach((filename) => {
    const fileDir = path.resolve(dir, filename);
    const stats = fse.statSync(fileDir);
    if (stats.isDirectory()) {
      // 删除文件
      scan(fileDir, remove);
      // 删除空的文件夹
      if (fse.readdirSync(fileDir).length == 0) {
        fse.rmdirSync(fileDir);
      }
      return;
    }
    if (callback) {
      callback(fileDir, stats);
    }
  });
}

// * * * * * *
// ┬ ┬ ┬ ┬ ┬ ┬
// │ │ │ │ │ │
// │ │ │ │ │ └ day of week (0 - 7) (0 or 7 is Sun)
// │ │ │ │ └───── month (1 - 12)
// │ │ │ └────────── day of month (1 - 31)
// │ │ └─────────────── hour (0 - 23)
// │ └──────────────────── minute (0 - 59)
// └───────────────────────── second (0 - 59, OPTIONAL)
let start = function () {
  // 每5秒
  schedule.scheduleJob('*/5 * * * * *', function () {
    console.log('定时清理chunks开始');
    scan(UPLOAD_DIR);
  });
};

start();
```

server\target 空目录，用于存放上传文件







